{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 04 - Model Evaluation & Interpretability\n\nThis notebook provides:\n1. Final test set evaluation (dollar-scale via inverse log transform)\n2. Feature importance analysis (all tree-based models)\n3. SHAP explanations (global and local)\n4. Prediction error analysis\n5. Business logic validation checks\n6. Model recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Run: pip install shap\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load test data (log-scale targets for prediction, original for evaluation)\nX_test = pd.read_csv('../data/processed/X_test.csv')\nX_test_scaled = pd.read_csv('../data/processed/X_test_scaled.csv')\ny_test = pd.read_csv('../data/processed/y_test.csv').squeeze()           # log-scale\ny_test_original = pd.read_csv('../data/processed/y_test_original.csv').squeeze()  # dollar-scale\n\n# Load validation data\nX_val = pd.read_csv('../data/processed/X_val.csv')\nX_val_scaled = pd.read_csv('../data/processed/X_val_scaled.csv')\ny_val = pd.read_csv('../data/processed/y_val.csv').squeeze()\ny_val_original = pd.read_csv('../data/processed/y_val_original.csv').squeeze()\n\n# Load all models\nlr_model = joblib.load('../models/linear_regression_model.joblib')\nridge_model = joblib.load('../models/ridge_model.joblib')\nlasso_model = joblib.load('../models/lasso_model.joblib')\ndt_model = joblib.load('../models/decision_tree_model.joblib')\nrf_model = joblib.load('../models/random_forest_model.joblib')\ngb_model = joblib.load('../models/gradient_boosting_model.joblib')\n\n# Load comparison results\nresults_df = pd.read_csv('../models/model_comparison.csv')\nprint(\"Model Comparison (from validation set):\")\nprint(results_df[['model', 'MAE', 'RMSE', 'R2']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Predictions on test set — predict in log-space then inverse-transform to dollars\ndef predict_dollars(model, X):\n    \"\"\"Predict in log-space and convert back to dollars.\"\"\"\n    return np.expm1(model.predict(X))\n\n# Best tree model for detailed analysis\nbest_tree_model = rf_model\nbest_tree_name = \"Random Forest\"\n\ny_pred_tree = predict_dollars(best_tree_model, X_test)\ny_pred_ridge = predict_dollars(ridge_model, X_test_scaled)\ny_pred_gb = predict_dollars(gb_model, X_test)\n\n# Use original dollar-scale actuals\ny_actual = y_test_original.values\n\n# Calculate errors in dollar terms\nerrors_tree = y_actual - y_pred_tree\nerrors_ridge = y_actual - y_pred_ridge\nerrors_gb = y_actual - y_pred_gb\n\n# Print test-set metrics\nprint(\"TEST SET EVALUATION (dollar scale)\")\nprint(\"=\"*60)\nfor name, y_pred in [(\"Ridge\", y_pred_ridge), (best_tree_name, y_pred_tree), (\"Gradient Boosting\", y_pred_gb)]:\n    mae = mean_absolute_error(y_actual, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_actual, y_pred))\n    r2 = r2_score(y_actual, y_pred)\n    print(f\"  {name:20s}  MAE=${mae:>10,.0f}  RMSE=${rmse:>10,.0f}  R²={r2:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Error distributions (dollar scale)\naxes[0, 0].hist(errors_ridge, bins=50, edgecolor='black', alpha=0.7)\naxes[0, 0].axvline(0, color='red', linestyle='--')\naxes[0, 0].set_xlabel('Prediction Error ($)')\naxes[0, 0].set_title('Ridge Regression - Error Distribution')\n\naxes[0, 1].hist(errors_tree, bins=50, edgecolor='black', alpha=0.7)\naxes[0, 1].axvline(0, color='red', linestyle='--')\naxes[0, 1].set_xlabel('Prediction Error ($)')\naxes[0, 1].set_title(f'{best_tree_name} - Error Distribution')\n\naxes[0, 2].hist(errors_gb, bins=50, edgecolor='black', alpha=0.7)\naxes[0, 2].axvline(0, color='red', linestyle='--')\naxes[0, 2].set_xlabel('Prediction Error ($)')\naxes[0, 2].set_title('Gradient Boosting - Error Distribution')\n\n# Predicted vs Actual (dollar scale)\naxes[1, 0].scatter(y_actual, y_pred_ridge, alpha=0.3)\naxes[1, 0].plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'r--', lw=2)\naxes[1, 0].set_xlabel('Actual Salary ($)')\naxes[1, 0].set_ylabel('Predicted Salary ($)')\naxes[1, 0].set_title('Ridge - Predicted vs Actual')\n\naxes[1, 1].scatter(y_actual, y_pred_tree, alpha=0.3)\naxes[1, 1].plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'r--', lw=2)\naxes[1, 1].set_xlabel('Actual Salary ($)')\naxes[1, 1].set_ylabel('Predicted Salary ($)')\naxes[1, 1].set_title(f'{best_tree_name} - Predicted vs Actual')\n\naxes[1, 2].scatter(y_actual, y_pred_gb, alpha=0.3)\naxes[1, 2].plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'r--', lw=2)\naxes[1, 2].set_xlabel('Actual Salary ($)')\naxes[1, 2].set_ylabel('Predicted Salary ($)')\naxes[1, 2].set_title('Gradient Boosting - Predicted vs Actual')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance (Tree-Based Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest feature importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Features (Random Forest):\")\n",
    "print(rf_importance.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20 = rf_importance.head(20)\n",
    "plt.barh(top_20['feature'], top_20['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Decision Tree feature importance\ndt_importance = pd.DataFrame({\n    'feature': X_test.columns,\n    'importance': dt_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 8))\ntop_20_dt = dt_importance.head(20)\nplt.barh(top_20_dt['feature'], top_20_dt['importance'])\nplt.xlabel('Feature Importance')\nplt.title('Top 20 Features - Decision Tree')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n# Gradient Boosting feature importance\ngb_importance = pd.DataFrame({\n    'feature': X_test.columns,\n    'importance': gb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 8))\ntop_20_gb = gb_importance.head(20)\nplt.barh(top_20_gb['feature'], top_20_gb['importance'])\nplt.xlabel('Feature Importance')\nplt.title('Top 20 Features - Gradient Boosting')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lasso Coefficients (Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso coefficients (non-zero = selected features)\n",
    "lasso_coefs = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'coefficient': lasso_model.coef_\n",
    "})\n",
    "lasso_coefs['abs_coef'] = lasso_coefs['coefficient'].abs()\n",
    "lasso_coefs = lasso_coefs.sort_values('abs_coef', ascending=False)\n",
    "\n",
    "print(f\"Non-zero coefficients: {(lasso_coefs['coefficient'] != 0).sum()} / {len(lasso_coefs)}\")\n",
    "print(\"\\nTop 20 Features by Absolute Coefficient (Lasso):\")\n",
    "print(lasso_coefs.head(20)[['feature', 'coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top Lasso coefficients\n",
    "top_20_lasso = lasso_coefs.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['green' if c > 0 else 'red' for c in top_20_lasso['coefficient']]\n",
    "plt.barh(top_20_lasso['feature'], top_20_lasso['coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Top 20 Features - Lasso Coefficients\\n(Green = positive impact, Red = negative impact)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    print(\"Computing SHAP values (this may take a few minutes)...\")\n",
    "    \n",
    "    # Use a sample for faster computation\n",
    "    sample_size = min(500, len(X_test))\n",
    "    X_sample = X_test.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Create SHAP explainer for Random Forest\n",
    "    explainer_rf = shap.TreeExplainer(rf_model)\n",
    "    shap_values_rf = explainer_rf.shap_values(X_sample)\n",
    "    \n",
    "    print(\"SHAP values computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # SHAP Summary Plot (Global Explanation)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values_rf, X_sample, plot_type=\"bar\", max_display=20, show=False)\n",
    "    plt.title('SHAP Feature Importance (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # SHAP Summary Plot with direction\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values_rf, X_sample, max_display=20, show=False)\n",
    "    plt.title('SHAP Summary Plot - Feature Impact Direction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Local Explanation - Waterfall plot for a single prediction\n",
    "    print(\"\\nLocal Explanation - Example Prediction\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Select a sample\n",
    "    idx = 0\n",
    "    sample = X_sample.iloc[idx:idx+1]\n",
    "    actual = y_test.iloc[X_sample.index[idx]]\n",
    "    predicted = rf_model.predict(sample)[0]\n",
    "    \n",
    "    print(f\"Actual Salary: ${actual:,.2f}\")\n",
    "    print(f\"Predicted Salary: ${predicted:,.2f}\")\n",
    "    print(f\"Error: ${actual - predicted:,.2f}\")\n",
    "    \n",
    "    # Waterfall plot\n",
    "    shap.plots.waterfall(shap.Explanation(\n",
    "        values=shap_values_rf[idx],\n",
    "        base_values=explainer_rf.expected_value,\n",
    "        data=X_sample.iloc[idx],\n",
    "        feature_names=X_sample.columns.tolist()\n",
    "    ), max_display=15, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze errors by salary range (using dollar-scale values)\nerror_analysis = pd.DataFrame({\n    'actual': y_actual,\n    'predicted': y_pred_tree,\n    'error': errors_tree,\n    'abs_error': np.abs(errors_tree)\n})\n\n# Bin salaries\nerror_analysis['salary_bin'] = pd.cut(error_analysis['actual'], \n                                       bins=[0, 75000, 125000, 175000, 500000],\n                                       labels=['<75K', '75K-125K', '125K-175K', '>175K'])\n\n# MAE by salary range\nmae_by_range = error_analysis.groupby('salary_bin')['abs_error'].agg(['mean', 'std', 'count'])\nmae_by_range.columns = ['MAE', 'Std', 'Count']\nprint(f\"MAE by Salary Range ({best_tree_name}):\")\nprint(mae_by_range)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MAE by salary range\n",
    "plt.figure(figsize=(10, 6))\n",
    "mae_by_range['MAE'].plot(kind='bar')\n",
    "plt.xlabel('Salary Range')\n",
    "plt.ylabel('Mean Absolute Error ($)')\n",
    "plt.title(f'Prediction Error by Salary Range ({best_tree_name})')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Business Logic Validation\n\nSanity-check that models produce economically sensible predictions:\n- **Test 1**: Increasing `years_experience` (0→19) should monotonically increase predicted salary\n- **Test 2**: Increasing `experience_level_enc` (EN→MI→SE→EX) should increase predicted salary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Build a baseline row from median of training data\nbaseline_row = X_test.median()\n\n# Models to test — tuple of (name, model, uses_scaled_features)\nmodels_to_check = [\n    ('Linear Regression', lr_model, True),\n    ('Ridge', ridge_model, True),\n    ('Lasso', lasso_model, True),\n    ('Decision Tree', dt_model, False),\n    ('Random Forest', rf_model, False),\n    ('Gradient Boosting', gb_model, False),\n]\n\n# Load scaler for scaled baseline\nartifacts = joblib.load('../data/processed/preprocessing_artifacts.joblib')\nscaler = artifacts['scaler']\ncols_to_scale = artifacts['cols_to_scale']\n\ndef make_row(base, overrides, scaled=False):\n    \"\"\"Create a single-row DataFrame with overrides applied.\"\"\"\n    row = base.copy()\n    for k, v in overrides.items():\n        row[k] = v\n    df = pd.DataFrame([row])\n    if scaled:\n        df_scaled = df.copy()\n        df_scaled[cols_to_scale] = scaler.transform(df[cols_to_scale])\n        return df_scaled\n    return df\n\n# ---- Test 1: Vary years_experience 0→19 ----\nyears_range = list(range(0, 20))\ntest1_results = {}\n\nfor name, model, uses_scaled in models_to_check:\n    preds = []\n    for yr in years_range:\n        row = make_row(baseline_row, {'years_experience': yr}, scaled=uses_scaled)\n        pred_dollar = np.expm1(model.predict(row))[0]\n        preds.append(pred_dollar)\n    test1_results[name] = preds\n\n# ---- Test 2: Vary experience_level_enc EN(0)→MI(1)→SE(2)→EX(3) ----\nexp_levels = [0, 1, 2, 3]\nexp_labels = ['EN', 'MI', 'SE', 'EX']\ntest2_results = {}\n\nfor name, model, uses_scaled in models_to_check:\n    preds = []\n    for lvl in exp_levels:\n        row = make_row(baseline_row, {'experience_level_enc': lvl}, scaled=uses_scaled)\n        pred_dollar = np.expm1(model.predict(row))[0]\n        preds.append(pred_dollar)\n    test2_results[name] = preds\n\n# ---- Check monotonicity ----\ndef is_monotonic_increasing(values):\n    return all(values[i] <= values[i+1] for i in range(len(values) - 1))\n\nprint(\"=\"*70)\nprint(\"BUSINESS LOGIC VALIDATION RESULTS\")\nprint(\"=\"*70)\n\nbl_results = []\nfor name in [m[0] for m in models_to_check]:\n    t1_pass = is_monotonic_increasing(test1_results[name])\n    t2_pass = is_monotonic_increasing(test2_results[name])\n    status1 = \"PASS\" if t1_pass else \"FAIL\"\n    status2 = \"PASS\" if t2_pass else \"FAIL\"\n    bl_results.append({'Model': name, 'Test1_YearsExp': status1, 'Test2_ExpLevel': status2})\n    print(f\"  {name:25s}  years_exp monotonic: {status1}  |  exp_level monotonic: {status2}\")\n\nbl_df = pd.DataFrame(bl_results)\nprint(\"\\n\" + bl_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---- Visualize Business Logic Tests ----\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Test 1: Years of experience line plot\nax1 = axes[0]\nfor name in [m[0] for m in models_to_check]:\n    ax1.plot(years_range, test1_results[name], marker='o', markersize=3, label=name)\nax1.set_xlabel('Years of Experience')\nax1.set_ylabel('Predicted Salary ($)')\nax1.set_title('Test 1: Predicted Salary vs Years of Experience')\nax1.legend(fontsize=8)\nax1.grid(True, alpha=0.3)\n\n# Test 2: Experience level bar plot\nax2 = axes[1]\nx_pos = np.arange(len(exp_labels))\nwidth = 0.1\nfor i, name in enumerate([m[0] for m in models_to_check]):\n    offset = (i - len(models_to_check) / 2) * width\n    ax2.bar(x_pos + offset, test2_results[name], width, label=name)\nax2.set_xlabel('Experience Level')\nax2.set_ylabel('Predicted Salary ($)')\nax2.set_title('Test 2: Predicted Salary vs Experience Level')\nax2.set_xticks(x_pos)\nax2.set_xticklabels(exp_labels)\nax2.legend(fontsize=8)\nax2.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Print summary\nn_pass_t1 = sum(1 for r in bl_results if r['Test1_YearsExp'] == 'PASS')\nn_pass_t2 = sum(1 for r in bl_results if r['Test2_ExpLevel'] == 'PASS')\nn_total = len(bl_results)\nprint(f\"\\nTest 1 (years_experience monotonic): {n_pass_t1}/{n_total} models passed\")\nprint(f\"Test 2 (experience_level monotonic):  {n_pass_t2}/{n_total} models passed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Final Summary & Recommendations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"FINAL MODEL EVALUATION SUMMARY\")\nprint(\"=\"*70)\n\nprint(\"\\n1. MODEL PERFORMANCE RANKING:\")\nprint(\"-\"*40)\nfor i, row in results_df.iterrows():\n    print(f\"   {i+1}. {row['model']}: MAE=${row['MAE']:,.0f}, R²={row['R2']:.4f}\")\n\nprint(\"\\n2. KEY FINDINGS:\")\nprint(\"-\"*40)\nprint(\"   - Tree-based models (Random Forest, Gradient Boosting) typically outperform linear models\")\nprint(\"   - Log-transform of salary + inverse-transform evaluation ensures dollar-scale metrics\")\nprint(\"   - Top predictive features include:\")\nprint(\"     * Experience level and years of experience\")\nprint(\"     * Job title\")\nprint(\"     * Company location (country)\")\nprint(\"     * Technical skills (especially cloud/ML skills)\")\n\nprint(\"\\n3. BUSINESS LOGIC VALIDATION:\")\nprint(\"-\"*40)\nn_pass_t1 = sum(1 for r in bl_results if r['Test1_YearsExp'] == 'PASS')\nn_pass_t2 = sum(1 for r in bl_results if r['Test2_ExpLevel'] == 'PASS')\nn_total = len(bl_results)\nprint(f\"   - Test 1 (salary increases with years_experience): {n_pass_t1}/{n_total} models passed\")\nprint(f\"   - Test 2 (salary increases with experience_level): {n_pass_t2}/{n_total} models passed\")\nfor r in bl_results:\n    flag = \" \" if r['Test1_YearsExp'] == 'PASS' and r['Test2_ExpLevel'] == 'PASS' else \" *\"\n    print(f\"     {r['Model']:25s} T1={r['Test1_YearsExp']}  T2={r['Test2_ExpLevel']}{flag}\")\n\nprint(\"\\n4. RECOMMENDATIONS:\")\nprint(\"-\"*40)\nbest_model = results_df.iloc[0]['model']\nprint(f\"   - Use {best_model} for production deployment\")\nprint(\"   - Consider ensemble methods for improved robustness\")\nprint(\"   - Retrain periodically with updated salary data\")\nprint(\"   - For interpretability needs, use SHAP explanations\")\nprint(\"   - Models that fail business logic checks may need monotonic constraints\")\n\nprint(\"\\n5. LIMITATIONS:\")\nprint(\"-\"*40)\nprint(\"   - Dataset is synthetically generated\")\nprint(\"   - Real-world salary prediction requires verified data sources\")\nprint(\"   - Model should be validated on actual job market data before production use\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save evaluation artifacts\nimport os\nos.makedirs('../outputs', exist_ok=True)\n\n# Save feature importance\nrf_importance.to_csv('../outputs/feature_importance_rf.csv', index=False)\nlasso_coefs.to_csv('../outputs/lasso_coefficients.csv', index=False)\n\n# Save business logic results\nbl_df.to_csv('../outputs/business_logic_results.csv', index=False)\n\n# Save DT and GB feature importance\ndt_importance.to_csv('../outputs/feature_importance_dt.csv', index=False)\ngb_importance.to_csv('../outputs/feature_importance_gb.csv', index=False)\n\nprint(\"Evaluation artifacts saved to ../outputs/\")\nprint(\"  - feature_importance_rf.csv\")\nprint(\"  - feature_importance_dt.csv\")\nprint(\"  - feature_importance_gb.csv\")\nprint(\"  - lasso_coefficients.csv\")\nprint(\"  - business_logic_results.csv\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}