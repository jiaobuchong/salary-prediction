{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 03 - Model Training & Comparison\n\nThis notebook trains and compares multiple ML models:\n1. Linear Regression (baseline)\n2. Ridge Regression\n3. Lasso Regression\n4. Decision Tree\n5. Random Forest\n6. Gradient Boosting\n\nModels are trained on log-transformed salary and evaluated in original dollar scale.\nUses 5-fold cross-validation for model selection and hyperparameter tuning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load unscaled data (for tree-based models)\nX_train = pd.read_csv('../data/processed/X_train.csv')\nX_val = pd.read_csv('../data/processed/X_val.csv')\nX_test = pd.read_csv('../data/processed/X_test.csv')\n\n# Load log-scale targets (for training)\ny_train = pd.read_csv('../data/processed/y_train.csv').squeeze()\ny_val = pd.read_csv('../data/processed/y_val.csv').squeeze()\ny_test = pd.read_csv('../data/processed/y_test.csv').squeeze()\n\n# Load original-scale targets (for dollar-based evaluation)\ny_train_original = pd.read_csv('../data/processed/y_train_original.csv').squeeze()\ny_val_original = pd.read_csv('../data/processed/y_val_original.csv').squeeze()\ny_test_original = pd.read_csv('../data/processed/y_test_original.csv').squeeze()\n\n# Load scaled data (for linear models)\nX_train_scaled = pd.read_csv('../data/processed/X_train_scaled.csv')\nX_val_scaled = pd.read_csv('../data/processed/X_val_scaled.csv')\nX_test_scaled = pd.read_csv('../data/processed/X_test_scaled.csv')\n\nprint(f\"Training set:   {X_train.shape}\")\nprint(f\"Validation set: {X_val.shape}\")\nprint(f\"Test set:       {X_test.shape}\")\nprint(f\"\\nTarget is log-transformed salary (log1p). Sample y_train values:\")\nprint(y_train.head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, X_test, y_test_log, y_test_orig, model_name):\n    \"\"\"Evaluate model: predict in log-space, inverse-transform to dollars for metrics.\"\"\"\n    y_pred_log = model.predict(X_test)\n    # Inverse transform: expm1 undoes log1p\n    y_pred_dollars = np.expm1(y_pred_log)\n    y_actual_dollars = y_test_orig.values if hasattr(y_test_orig, 'values') else y_test_orig\n\n    mae = mean_absolute_error(y_actual_dollars, y_pred_dollars)\n    rmse = np.sqrt(mean_squared_error(y_actual_dollars, y_pred_dollars))\n    r2 = r2_score(y_actual_dollars, y_pred_dollars)\n\n    print(f\"\\n{model_name} Results (dollar scale):\")\n    print(f\"  MAE:  ${mae:,.2f}\")\n    print(f\"  RMSE: ${rmse:,.2f}\")\n    print(f\"  R²:   {r2:.4f}\")\n\n    return {'model': model_name, 'MAE': mae, 'RMSE': rmse, 'R2': r2}\n\n\n# Store results\nresults = []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Model 1: Linear Regression (Baseline)\n\nPlain linear regression with no regularization serves as the baseline model that all other models are compared against."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"Training Linear Regression (Baseline)...\")\nprint(\"=\"*60)\n\nlr_model = LinearRegression()\nlr_model.fit(X_train_scaled, y_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate baseline on validation set\nlr_results = evaluate_model(lr_model, X_val_scaled, y_val, y_val_original, \"Linear Regression\")\nresults.append(lr_results)\n\n# Store baseline MAE for comparison\nbaseline_mae = lr_results['MAE']\nbaseline_rmse = lr_results['RMSE']\nbaseline_r2 = lr_results['R2']\nprint(f\"\\n>>> Baseline MAE: ${baseline_mae:,.2f} — all models compared against this.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model 2: Ridge Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"Training Ridge Regression...\")\nprint(\"=\"*60)\n\nridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n\nridge_gs = GridSearchCV(\n    Ridge(), \n    ridge_params, \n    cv=5, \n    scoring='neg_mean_absolute_error',\n    n_jobs=1\n)\nridge_gs.fit(X_train_scaled, y_train)\n\nprint(f\"Best alpha: {ridge_gs.best_params_['alpha']}\")\n\nridge_model = ridge_gs.best_estimator_\ndel ridge_gs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on validation set\nridge_results = evaluate_model(ridge_model, X_val_scaled, y_val, y_val_original, \"Ridge Regression\")\nresults.append(ridge_results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Model 3: Lasso Regression"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"Training Lasso Regression...\")\nprint(\"=\"*60)\n\nlasso_params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]}\n\nlasso_gs = GridSearchCV(\n    Lasso(max_iter=10000), \n    lasso_params, \n    cv=5, \n    scoring='neg_mean_absolute_error',\n    n_jobs=1\n)\nlasso_gs.fit(X_train_scaled, y_train)\n\nprint(f\"Best alpha: {lasso_gs.best_params_['alpha']}\")\n\nlasso_model = lasso_gs.best_estimator_\ndel lasso_gs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on validation set\nlasso_results = evaluate_model(lasso_model, X_val_scaled, y_val, y_val_original, \"Lasso Regression\")\nresults.append(lasso_results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Model 4: Decision Tree"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"Training Decision Tree...\")\nprint(\"=\"*60)\n\ndt_params = {\n    'max_depth': [5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10, 20],\n    'min_samples_leaf': [1, 2, 4, 8]\n}\n\ndt_gs = GridSearchCV(\n    DecisionTreeRegressor(random_state=42),\n    dt_params,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    n_jobs=1\n)\ndt_gs.fit(X_train, y_train)\n\nprint(f\"Best parameters: {dt_gs.best_params_}\")\n\ndt_model = dt_gs.best_estimator_\ndel dt_gs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on validation set\ndt_results = evaluate_model(dt_model, X_val, y_val, y_val_original, \"Decision Tree\")\nresults.append(dt_results)"
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Model 5: Random Forest",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*60)\nprint(\"Training Random Forest...\")\nprint(\"=\"*60)\n\nrf_params = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf_rs = RandomizedSearchCV(\n    RandomForestRegressor(random_state=42),\n    rf_params,\n    n_iter=20,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    random_state=42,\n    n_jobs=1\n)\nrf_rs.fit(X_train, y_train)\n\nprint(f\"Best parameters: {rf_rs.best_params_}\")\n\nrf_model = rf_rs.best_estimator_\ndel rf_rs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate on validation set\nrf_results = evaluate_model(rf_model, X_val, y_val, y_val_original, \"Random Forest\")\nresults.append(rf_results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Model 6: Gradient Boosting"
  },
  {
   "cell_type": "code",
   "source": "print(\"=\"*60)\nprint(\"Training Gradient Boosting...\")\nprint(\"=\"*60)\n\ngb_params = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.8, 1.0],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngb_rs = RandomizedSearchCV(\n    GradientBoostingRegressor(random_state=42),\n    gb_params,\n    n_iter=20,\n    cv=5,\n    scoring='neg_mean_absolute_error',\n    random_state=42,\n    n_jobs=1\n)\ngb_rs.fit(X_train, y_train)\n\nprint(f\"Best parameters: {gb_rs.best_params_}\")\n\ngb_model = gb_rs.best_estimator_\ndel gb_rs",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Evaluate on validation set\ngb_results = evaluate_model(gb_model, X_val, y_val, y_val_original, \"Gradient Boosting\")\nresults.append(gb_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create results DataFrame\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('MAE')\n\n# Add improvement vs baseline columns\nresults_df['MAE_improvement'] = baseline_mae - results_df['MAE']\nresults_df['MAE_improvement_pct'] = (results_df['MAE_improvement'] / baseline_mae) * 100\nresults_df['R2_improvement'] = results_df['R2'] - baseline_r2\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON (sorted by MAE, dollar scale)\")\nprint(\"=\"*70)\nprint(results_df[['model', 'MAE', 'RMSE', 'R2']].to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"IMPROVEMENT vs LINEAR REGRESSION BASELINE\")\nprint(\"=\"*70)\nprint(results_df[['model', 'MAE_improvement', 'MAE_improvement_pct', 'R2_improvement']].to_string(index=False, float_format='%.2f'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.makedirs('../models', exist_ok=True)\n\n# Save all models\nmodels_dict = {\n    'linear_regression': lr_model,\n    'ridge': ridge_model,\n    'lasso': lasso_model,\n    'decision_tree': dt_model,\n    'random_forest': rf_model,\n    'gradient_boosting': gb_model,\n}\n\nfor name, model in models_dict.items():\n    joblib.dump(model, f'../models/{name}_model.joblib')\n    print(f\"Saved: ../models/{name}_model.joblib\")\n\n# Save comparison results\nresults_df.to_csv('../models/model_comparison.csv', index=False)\nprint(\"\\nSaved: ../models/model_comparison.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "best_mae = results_df.iloc[0]['MAE']\n",
    "best_r2 = results_df.iloc[0]['R2']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"  MAE:  ${best_mae:,.2f}\")\n",
    "print(f\"  R²:   {best_r2:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Models Trained (in order):\n1. **Linear Regression** - Baseline model (no regularization)\n2. **Ridge Regression** - L2 regularization\n3. **Lasso Regression** - L1 regularization (feature selection)\n4. **Decision Tree** - Single tree with GridSearchCV tuning\n5. **Random Forest** - Ensemble of decision trees\n6. **Gradient Boosting** - Sequential boosting (sklearn)\n\n### Key Outputs:\n- All models evaluated on **validation set** (dollar scale via `expm1` inverse transform)\n- **Baseline comparison table** showing MAE/R² improvement vs Linear Regression\n- **K-fold CV comparison table** for generalization assessment\n- All 6 trained models saved to `../models/`\n\n### Next Steps:\n- See `04_evaluation.ipynb` for detailed evaluation, feature importance, SHAP, and business logic checks"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}